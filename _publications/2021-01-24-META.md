---
title: "Accelerating Gradient-based Meta Learner"
collection: publications
permalink: /publication/2021-01-24-META
date: 2021-01-24
venue: 'International Conference on Performance Engineering'
pubtype: 'conference'
---

*Work under submission, more details will be shared once published.*

Meta Learning has been in focus in recent years due to the meta-learner modelâ€™s ability to adapt well and generalize to new tasks, thus, reducing both the time and data requirements for learning. However, a major drawback of meta learner is that, to reach to a state from where learning new tasks becomes feasible with less data, it requires a large number of iterations and a lot of time. We address this issue by proposing various acceleration techniques to speed up meta learning algorithms such as MAML (Model Agnostic Meta Learning). We present 3.73X acceleration on a well known
RNN optimizer based meta learner proposed in literature [11]. We introduce a novel method of training tasks in clusters, which not only accelerates the meta learning process but also improves model
accuracy performance.

**Additional Materials:** *to be updated soon...*